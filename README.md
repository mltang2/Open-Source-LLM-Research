# Open-Source-LLM-Research
With professor Dr. George Thomas, we spent the summer diving deep into how we can access open source large language models, fine tune them, and integrate them into existing applications. Here is what we found!

While it was possible to have applications access the interface programatically, it was easiest to incorporate applications that already had functionalities that could download and run different llms efficiently. After playing around with multiple platforms including UnionCloud, TensorFlow, and Ollama, we ultimately decided to continue with Ollama. The main reason for this choice was because of the ease of use that Ollama provided. With its potential for local execution with hundreds of open source models, mixed in with its endless possibilities for customization, versatility, and integration, we discovered that Ollama was the best platform to run large language models. 
https://github.com/ollama/ollama 

After a few days of running different llms like llama3, gemma, and mistral, we decided to look further into how we could access them on other devices without needing to redownload the llms and the ollama pplatofrm


